{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install pandas python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport os\nimport uuid\nfrom contextlib import contextmanager\nfrom dotenv import load_dotenv\nimport pandas as pd\n\n# Load environment variables\nload_dotenv()\n\n# Benchmarking functions\n@contextmanager\ndef timer():\n    \"\"\"Context manager to time execution\"\"\"\n    start_time = time.time()\n    yield\n    end_time = time.time()\n    execution_time = end_time - start_time\n    print(f\"Execution time: {execution_time:.2f} seconds\")\n\n# Configuration\nNUM_TRACES = 20\n\n# Hardcoded trace data for consistent benchmarking\nHARDCODED_TRACES = [\n    {\n        \"trace_id\": f\"trace_{i}\",\n        \"user_id\": f\"user_{(i % 10) + 1}\",\n        \"session_id\": f\"session_{(i % 5) + 1}\",\n        \"tool_call\": {\n            \"input_size\": 100 + (i * 50),\n            \"duration\": 0.005 + (i * 0.001)\n        },\n        \"llm_call\": {\n            \"provider\": \"openai\",\n            \"model\": \"gpt-4\",\n            \"temperature\": 0.7,\n            \"input\": f\"Analyze the data for request {i}\",\n            \"output\": f\"Analysis complete for request {i}: The result shows pattern {i % 3}\",\n            \"prompt_tokens\": 20 + (i % 30),\n            \"completion_tokens\": 50 + (i % 80),\n            \"total_tokens\": 70 + (i % 110),\n            \"duration\": 0.1 + (i * 0.01)\n        },\n        \"result\": {\n            \"output_format\": \"json\",\n            \"duration\": 0.002 + (i * 0.0005)\n        }\n    }\n    for i in range(NUM_TRACES) \n]\n\ndef run_benchmark(platform_name, log_function, num_traces=NUM_TRACES):\n    \"\"\"Generic benchmark runner\"\"\"\n    print(f\"Logging {num_traces} traces to {platform_name}...\")\n    \n    with timer():\n        for i in range(num_traces):\n            trace_data = HARDCODED_TRACES[i]\n            log_function(trace_data)\n            \n            if (i + 1) % 10 == 0:\n                print(f\"Logged {i + 1} traces...\")\n    \n    print(f\"Successfully logged {num_traces} traces to {platform_name}!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded trace data for consistent benchmarking\n",
    "HARDCODED_TRACES = [\n",
    "    {\n",
    "        \"trace_id\": f\"trace_{i}\",\n",
    "        \"user_id\": f\"user_{(i % 10) + 1}\",\n",
    "        \"session_id\": f\"session_{(i % 5) + 1}\",\n",
    "        \"tool_call\": {\n",
    "            \"input_size\": 100 + (i * 50),\n",
    "            \"duration\": 0.005 + (i * 0.001)\n",
    "        },\n",
    "        \"llm_call\": {\n",
    "            \"provider\": \"openai\",\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"input\": f\"Analyze the data for request {i}\",\n",
    "            \"output\": f\"Analysis complete for request {i}: The result shows pattern {i % 3}\",\n",
    "            \"prompt_tokens\": 20 + (i % 30),\n",
    "            \"completion_tokens\": 50 + (i % 80),\n",
    "            \"total_tokens\": 70 + (i % 110),\n",
    "            \"duration\": 0.1 + (i * 0.01)\n",
    "        },\n",
    "        \"result\": {\n",
    "            \"output_format\": \"json\",\n",
    "            \"duration\": 0.002 + (i * 0.0005)\n",
    "        }\n",
    "    }\n",
    "    for i in range(NUM_TRACES) \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install opentelemetry-exporter-otlp opentelemetry-api openinference-semantic-conventions opentelemetry-sdk arize-otel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.otel import register\n",
    "from opentelemetry import trace\n",
    "\n",
    "# Configuration for span logging\n",
    "ARIZE_SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\")\n",
    "ARIZE_API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
    "\n",
    "\n",
    "# Setup OTel via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id = ARIZE_SPACE_ID, # in app space settings page\n",
    "    api_key = ARIZE_API_KEY, # in app space settings page\n",
    "    project_name = \"ADB-Benchmarks\", # name this to whatever you would like\n",
    ")\n",
    "\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_trace(tracer, trace_data):\n",
    "    \"\"\"Simplified function to log a single trace using hardcoded data\"\"\"\n",
    "    with tracer.start_as_current_span(trace_data[\"trace_id\"]) as root_span:\n",
    "        # Set trace-level attributes\n",
    "        root_span.set_attribute(\"user.id\", trace_data[\"user_id\"])\n",
    "        root_span.set_attribute(\"session.id\", trace_data[\"session_id\"])\n",
    "        \n",
    "        # Tool call span\n",
    "        with tracer.start_as_current_span(\"tool_call\") as tool_span:\n",
    "            tool_span.set_attribute(\"operation.type\", \"tool_call\")\n",
    "            tool_span.set_attribute(\"input.size\", trace_data[\"tool_call\"][\"input_size\"])\n",
    "            tool_span.add_event(\"tool_call_started\")\n",
    "        \n",
    "        # LLM call span\n",
    "        with tracer.start_as_current_span(\"llm_call\") as llm_span:\n",
    "            llm_data = trace_data[\"llm_call\"]\n",
    "            llm_span.set_attribute(\"llm.provider\", llm_data[\"provider\"])\n",
    "            llm_span.set_attribute(\"llm.model_name\", llm_data[\"model\"])\n",
    "            llm_span.set_attribute(\"llm.temperature\", llm_data[\"temperature\"])\n",
    "            llm_span.set_attribute(\"input.value\", llm_data[\"input\"])\n",
    "            llm_span.set_attribute(\"output.value\", llm_data[\"output\"])\n",
    "            llm_span.set_attribute(\"llm.token_count.prompt\", llm_data[\"prompt_tokens\"])\n",
    "            llm_span.set_attribute(\"llm.token_count.completion\", llm_data[\"completion_tokens\"])\n",
    "            llm_span.set_attribute(\"llm.token_count.total\", llm_data[\"total_tokens\"])\n",
    "\n",
    "        \n",
    "        # Result span\n",
    "        with tracer.start_as_current_span(\"result\") as result_span:\n",
    "            result_span.set_attribute(\"operation.type\", \"result\")\n",
    "            result_span.set_attribute(\"output.format\", trace_data[\"result\"][\"output_format\"])\n",
    "\n",
    "def benchmark_trace_logging(tracer, num_traces):\n",
    "    print(f\"Logging {num_traces} traces...\")\n",
    "    \n",
    "    with timer():\n",
    "        for i in range(num_traces):\n",
    "            # Use modulo to cycle through hardcoded traces if we need more than 100\n",
    "            trace_data = HARDCODED_TRACES[i % len(HARDCODED_TRACES)]\n",
    "            log_trace(tracer, trace_data)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Logged {i + 1} traces...\")\n",
    "    \n",
    "    print(f\"Successfully logged {num_traces} traces!\")\n",
    "\n",
    "# Run the benchmark\n",
    "benchmark_trace_logging(tracer, NUM_TRACES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install \"langfuse<3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# Initialize Langfuse client\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "LANGFUSE_HOST = \"https://us.cloud.langfuse.com\"\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=LANGFUSE_SECRET_KEY,\n",
    "    host=LANGFUSE_HOST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_trace_langfuse(langfuse, trace_data):\n",
    "    \"\"\"Log a single trace to Langfuse using hardcoded data\"\"\"\n",
    "    # Create trace using context manager\n",
    "    with langfuse.start_as_current_span(\n",
    "        name=trace_data[\"trace_id\"],\n",
    "        input={\"trace_id\": trace_data[\"trace_id\"]}\n",
    "    ) as trace:\n",
    "        \n",
    "        # Set trace-level attributes\n",
    "        trace.update_trace(\n",
    "            user_id=trace_data[\"user_id\"],\n",
    "            session_id=trace_data[\"session_id\"]\n",
    "        )\n",
    "        \n",
    "        # Tool call span\n",
    "        with langfuse.start_as_current_span(\n",
    "            name=\"tool_call\",\n",
    "            input={\"size\": trace_data[\"tool_call\"][\"input_size\"]}\n",
    "        ) as tool_span:\n",
    "            tool_span.update(\n",
    "                metadata={\n",
    "                    \"operation.type\": \"tool_call\",\n",
    "                    \"input.size\": trace_data[\"tool_call\"][\"input_size\"]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        \n",
    "        # LLM generation\n",
    "        llm_data = trace_data[\"llm_call\"]\n",
    "        with langfuse.start_as_current_generation(\n",
    "            name=\"llm_call\",\n",
    "            model=llm_data[\"model\"],\n",
    "            model_parameters={\n",
    "                \"temperature\": llm_data[\"temperature\"]\n",
    "            },\n",
    "            input=llm_data[\"input\"]\n",
    "        ) as generation:\n",
    "            \n",
    "            \n",
    "            generation.update(\n",
    "                output=llm_data[\"output\"],\n",
    "                usage_details={\n",
    "                    \"input_tokens\": llm_data[\"prompt_tokens\"],\n",
    "                    \"output_tokens\": llm_data[\"completion_tokens\"],\n",
    "                    \"total_tokens\": llm_data[\"total_tokens\"]\n",
    "                },\n",
    "                metadata={\n",
    "                    \"provider\": llm_data[\"provider\"]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Result span\n",
    "        with langfuse.start_as_current_span(\n",
    "            name=\"result\",\n",
    "            output={\"format\": trace_data[\"result\"][\"output_format\"]}\n",
    "        ) as result_span:\n",
    "            result_span.update(\n",
    "                metadata={\n",
    "                    \"operation.type\": \"result\",\n",
    "                    \"output.format\": trace_data[\"result\"][\"output_format\"]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "\n",
    "def benchmark_langfuse_logging(langfuse, num_traces):\n",
    "    \"\"\"Benchmark function to log traces to Langfuse\"\"\"\n",
    "    print(f\"Logging {num_traces} traces to Langfuse...\")\n",
    "    \n",
    "    with timer():\n",
    "        for i in range(num_traces):\n",
    "            # Use modulo to cycle through hardcoded traces\n",
    "            trace_data = HARDCODED_TRACES[i % len(HARDCODED_TRACES)]\n",
    "            log_trace_langfuse(langfuse, trace_data)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Logged {i + 1} traces...\")\n",
    "    \n",
    "    # Flush to ensure all data is sent\n",
    "    langfuse.flush()\n",
    "    print(f\"Successfully logged {num_traces} traces to Langfuse!\")\n",
    "\n",
    "# Run the Langfuse benchmark\n",
    "benchmark_langfuse_logging(langfuse, NUM_TRACES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braintrust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install braintrust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braintrust\n",
    "from braintrust import start_span\n",
    "\n",
    "# Initialize Braintrust logger\n",
    "BRAINTRUST_API_KEY = os.getenv(\"BRAINTRUST_API_KEY\")\n",
    "\n",
    "# Initialize logger for the project\n",
    "logger = braintrust.init_logger(\n",
    "    project_name=\"Testing\",\n",
    "    api_key=BRAINTRUST_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@braintrust.traced\n",
    "def log_trace_braintrust(trace_data):\n",
    "    \"\"\"Log a single trace to Braintrust using hardcoded data\"\"\"\n",
    "    \n",
    "    # Tool call span\n",
    "    with start_span(name=\"tool_call\") as tool_span:\n",
    "        tool_span.log({\n",
    "            \"input\": {\"size\": trace_data[\"tool_call\"][\"input_size\"]},\n",
    "            \"metadata\": {\n",
    "                \"operation.type\": \"tool_call\",\n",
    "                \"input.size\": trace_data[\"tool_call\"][\"input_size\"]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Simulate tool call duration\n",
    "        time.sleep(trace_data[\"tool_call\"][\"duration\"])\n",
    "        \n",
    "        tool_span.log({\n",
    "            \"output\": {\"status\": \"completed\"},\n",
    "            \"metrics\": {\n",
    "                \"duration\": trace_data[\"tool_call\"][\"duration\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # LLM generation span\n",
    "    with start_span(name=\"llm_call\") as llm_span:\n",
    "        llm_data = trace_data[\"llm_call\"]\n",
    "        \n",
    "        llm_span.log({\n",
    "            \"input\": llm_data[\"input\"],\n",
    "            \"metadata\": {\n",
    "                \"provider\": llm_data[\"provider\"],\n",
    "                \"model\": llm_data[\"model\"],\n",
    "                \"temperature\": llm_data[\"temperature\"]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Simulate LLM call duration\n",
    "        time.sleep(llm_data[\"duration\"])\n",
    "        \n",
    "        llm_span.log({\n",
    "            \"output\": llm_data[\"output\"],\n",
    "            \"metrics\": {\n",
    "                \"prompt_tokens\": llm_data[\"prompt_tokens\"],\n",
    "                \"completion_tokens\": llm_data[\"completion_tokens\"],\n",
    "                \"total_tokens\": llm_data[\"total_tokens\"],\n",
    "                \"duration\": llm_data[\"duration\"]\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Result span\n",
    "    with start_span(name=\"result\") as result_span:\n",
    "        result_span.log({\n",
    "            \"input\": {\"format_requested\": trace_data[\"result\"][\"output_format\"]},\n",
    "            \"metadata\": {\n",
    "                \"operation.type\": \"result\",\n",
    "                \"output.format\": trace_data[\"result\"][\"output_format\"]\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Simulate result processing duration\n",
    "        time.sleep(trace_data[\"result\"][\"duration\"])\n",
    "        \n",
    "        result_span.log({\n",
    "            \"output\": {\"format\": trace_data[\"result\"][\"output_format\"]},\n",
    "            \"metrics\": {\n",
    "                \"duration\": trace_data[\"result\"][\"duration\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "def benchmark_braintrust_logging(num_traces):\n",
    "    \"\"\"Benchmark function to log traces to Braintrust\"\"\"\n",
    "    print(f\"Logging {num_traces} traces to Braintrust...\")\n",
    "    \n",
    "    with timer():\n",
    "        for i in range(num_traces):\n",
    "            # Use modulo to cycle through hardcoded traces\n",
    "            trace_data = HARDCODED_TRACES[i % len(HARDCODED_TRACES)]\n",
    "            \n",
    "            # Create a trace with user and session context\n",
    "            with start_span(\n",
    "                name=trace_data[\"trace_id\"],\n",
    "                span_attributes={\n",
    "                    \"user_id\": trace_data[\"user_id\"],\n",
    "                    \"session_id\": trace_data[\"session_id\"]\n",
    "                }\n",
    "            ):\n",
    "                log_trace_braintrust(trace_data)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Logged {i + 1} traces...\")\n",
    "    \n",
    "    print(f\"Successfully logged {num_traces} traces to Braintrust!\")\n",
    "\n",
    "# Run the Braintrust benchmark\n",
    "benchmark_braintrust_logging(NUM_TRACES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}