{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install pandas python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import uuid\n",
    "from contextlib import contextmanager\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Benchmarking functions\n",
    "@contextmanager\n",
    "def timer():\n",
    "    \"\"\"Context manager to time execution\"\"\"\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Configuration\n",
    "NUM_TRACES = 20\n",
    "\n",
    "# Hardcoded trace data for consistent benchmarking\n",
    "HARDCODED_TRACES = [\n",
    "    {\n",
    "        \"trace_id\": f\"trace_{i}\",\n",
    "        \"user_id\": f\"user_{(i % 10) + 1}\",\n",
    "        \"session_id\": f\"session_{(i % 5) + 1}\",\n",
    "        \"tool_call\": {\n",
    "            \"input_size\": 100 + (i * 50),\n",
    "            \"duration\": 0.005 + (i * 0.001)\n",
    "        },\n",
    "        \"llm_call\": {\n",
    "            \"provider\": \"openai\",\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"input\": f\"Analyze the data for request {i}\",\n",
    "            \"output\": f\"Analysis complete for request {i}: The result shows pattern {i % 3}\",\n",
    "            \"prompt_tokens\": 20 + (i % 30),\n",
    "            \"completion_tokens\": 50 + (i % 80),\n",
    "            \"total_tokens\": 70 + (i % 110),\n",
    "            \"duration\": 0.1 + (i * 0.01)\n",
    "        },\n",
    "        \"result\": {\n",
    "            \"output_format\": \"json\",\n",
    "            \"duration\": 0.002 + (i * 0.0005)\n",
    "        }\n",
    "    }\n",
    "    for i in range(NUM_TRACES) \n",
    "]\n",
    "\n",
    "def run_benchmark(platform_name, log_function, num_traces=NUM_TRACES):\n",
    "    print(f\"Logging {num_traces} traces to {platform_name}...\")\n",
    "    \n",
    "    with timer():\n",
    "        for i in range(num_traces):\n",
    "            trace_data = HARDCODED_TRACES[i]\n",
    "            log_function(trace_data)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Logged {i + 1} traces...\")\n",
    "    \n",
    "    print(f\"Successfully logged {num_traces} traces to {platform_name}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install opentelemetry-exporter-otlp opentelemetry-api openinference-semantic-conventions opentelemetry-sdk arize-otel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Arize Project: ADB-Benchmarks\n",
      "|  Span Processor: BatchSpanProcessor\n",
      "|  Collector Endpoint: otlp.arize.com\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'authorization': '****', 'api_key': '****', 'arize-space-id': '****', 'space_id': '****', 'arize-interface': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from arize.otel import register\n",
    "from opentelemetry import trace\n",
    "\n",
    "# Configuration for span logging\n",
    "ARIZE_SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\")\n",
    "ARIZE_API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
    "\n",
    "\n",
    "# Setup OTel via our convenience function\n",
    "tracer_provider = register(\n",
    "    space_id = ARIZE_SPACE_ID, # in app space settings page\n",
    "    api_key = ARIZE_API_KEY, # in app space settings page\n",
    "    project_name = \"ADB-Benchmarks\", # name this to whatever you would like\n",
    ")\n",
    "\n",
    "trace.set_tracer_provider(tracer_provider)\n",
    "tracer = trace.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging 20 traces to Arize...\n",
      "Logged 10 traces...\n",
      "Logged 20 traces...\n",
      "Execution time: 0.00 seconds\n",
      "Successfully logged 20 traces to Arize!\n"
     ]
    }
   ],
   "source": [
    "def log_trace_arize(trace_data):\n",
    "    \"\"\"Log a single trace using Arize\"\"\"\n",
    "    with tracer.start_as_current_span(trace_data[\"trace_id\"]) as root_span:\n",
    "        # Set trace-level attributes\n",
    "        root_span.set_attribute(\"user.id\", trace_data[\"user_id\"])\n",
    "        root_span.set_attribute(\"session.id\", trace_data[\"session_id\"])\n",
    "        \n",
    "        # Tool call span\n",
    "        with tracer.start_as_current_span(\"tool_call\") as tool_span:\n",
    "            tool_span.set_attribute(\"operation.type\", \"tool_call\")\n",
    "            tool_span.set_attribute(\"input.size\", trace_data[\"tool_call\"][\"input_size\"])\n",
    "            tool_span.add_event(\"tool_call_started\")\n",
    "        \n",
    "        # LLM call span\n",
    "        with tracer.start_as_current_span(\"llm_call\") as llm_span:\n",
    "            llm_data = trace_data[\"llm_call\"]\n",
    "            llm_span.set_attribute(\"llm.provider\", llm_data[\"provider\"])\n",
    "            llm_span.set_attribute(\"llm.model_name\", llm_data[\"model\"])\n",
    "            llm_span.set_attribute(\"llm.temperature\", llm_data[\"temperature\"])\n",
    "            llm_span.set_attribute(\"input.value\", llm_data[\"input\"])\n",
    "            llm_span.set_attribute(\"output.value\", llm_data[\"output\"])\n",
    "            llm_span.set_attribute(\"llm.token_count.prompt\", llm_data[\"prompt_tokens\"])\n",
    "            llm_span.set_attribute(\"llm.token_count.completion\", llm_data[\"completion_tokens\"])\n",
    "            llm_span.set_attribute(\"llm.token_count.total\", llm_data[\"total_tokens\"])\n",
    "        \n",
    "        # Result span\n",
    "        with tracer.start_as_current_span(\"result\") as result_span:\n",
    "            result_span.set_attribute(\"operation.type\", \"result\")\n",
    "            result_span.set_attribute(\"output.format\", trace_data[\"result\"][\"output_format\"])\n",
    "\n",
    "# Run the Arize benchmark\n",
    "run_benchmark(\"Arize\", log_trace_arize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install \"langfuse<3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse import Langfuse\n",
    "\n",
    "# Initialize Langfuse client\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "LANGFUSE_HOST = \"https://us.cloud.langfuse.com\"\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=LANGFUSE_SECRET_KEY,\n",
    "    host=LANGFUSE_HOST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_trace_langfuse(trace_data):\n",
    "    \"\"\"Log a single trace to Langfuse using hardcoded data\"\"\"\n",
    "    # Create trace using context manager\n",
    "    with langfuse.start_as_current_span(\n",
    "        name=trace_data[\"trace_id\"],\n",
    "        input={\"trace_id\": trace_data[\"trace_id\"]}\n",
    "    ) as trace:\n",
    "        \n",
    "        # Set trace-level attributes\n",
    "        trace.update_trace(\n",
    "            user_id=trace_data[\"user_id\"],\n",
    "            session_id=trace_data[\"session_id\"]\n",
    "        )\n",
    "        \n",
    "        # Tool call span\n",
    "        with langfuse.start_as_current_span(\n",
    "            name=\"tool_call\",\n",
    "            input={\"size\": trace_data[\"tool_call\"][\"input_size\"]}\n",
    "        ) as tool_span:\n",
    "            tool_span.update(\n",
    "                metadata={\n",
    "                    \"operation.type\": \"tool_call\",\n",
    "                    \"input.size\": trace_data[\"tool_call\"][\"input_size\"]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Simulate tool call duration\n",
    "            time.sleep(trace_data[\"tool_call\"][\"duration\"])\n",
    "        \n",
    "        # LLM Call\n",
    "        llm_data = trace_data[\"llm_call\"]\n",
    "        with langfuse.start_as_current_generation(\n",
    "            name=\"llm_call\",\n",
    "            model=llm_data[\"model\"],\n",
    "            model_parameters={\n",
    "                \"temperature\": llm_data[\"temperature\"]\n",
    "            },\n",
    "            input=llm_data[\"input\"]\n",
    "        ) as generation:\n",
    "            \n",
    "            # Simulate LLM call duration\n",
    "            time.sleep(llm_data[\"duration\"])\n",
    "            \n",
    "            generation.update(\n",
    "                output=llm_data[\"output\"],\n",
    "                usage_details={\n",
    "                    \"input_tokens\": llm_data[\"prompt_tokens\"],\n",
    "                    \"output_tokens\": llm_data[\"completion_tokens\"],\n",
    "                    \"total_tokens\": llm_data[\"total_tokens\"]\n",
    "                },\n",
    "                metadata={\n",
    "                    \"provider\": llm_data[\"provider\"]\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Result span\n",
    "        with langfuse.start_as_current_span(\n",
    "            name=\"result\",\n",
    "            output={\"format\": trace_data[\"result\"][\"output_format\"]}\n",
    "        ) as result_span:\n",
    "            result_span.update(\n",
    "                metadata={\n",
    "                    \"operation.type\": \"result\",\n",
    "                    \"output.format\": trace_data[\"result\"][\"output_format\"]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Simulate result processing duration\n",
    "            time.sleep(trace_data[\"result\"][\"duration\"])\n",
    "\n",
    "def log_trace_langfuse_wrapper(trace_data):\n",
    "    \"\"\"Wrapper to handle Langfuse flush\"\"\"\n",
    "    log_trace_langfuse(trace_data)\n",
    "\n",
    "# Run the Langfuse benchmark\n",
    "run_benchmark(\"Langfuse\", log_trace_langfuse_wrapper)\n",
    "langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braintrust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install braintrust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braintrust\n",
    "from braintrust import start_span\n",
    "\n",
    "# Initialize Braintrust logger\n",
    "BRAINTRUST_API_KEY = os.getenv(\"BRAINTRUST_API_KEY\")\n",
    "\n",
    "# Initialize logger for the project\n",
    "logger = braintrust.init_logger(\n",
    "    project=\"Testing\",\n",
    "    api_key=BRAINTRUST_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging 20 traces to Braintrust...\n",
      "Logged 10 traces...\n",
      "Logged 20 traces...\n",
      "Execution time: 5.21 seconds\n",
      "Successfully logged 20 traces to Braintrust!\n"
     ]
    }
   ],
   "source": [
    "@braintrust.traced\n",
    "def log_trace_braintrust(trace_data):\n",
    "    \"\"\"Log a single trace to Braintrust using hardcoded data\"\"\"\n",
    "    \n",
    "    # Tool call span\n",
    "    with start_span(name=\"tool_call\") as tool_span:\n",
    "        tool_span.log(\n",
    "            input={\"size\": trace_data[\"tool_call\"][\"input_size\"]},\n",
    "            metadata={\n",
    "                \"operation.type\": \"tool_call\",\n",
    "                \"input.size\": trace_data[\"tool_call\"][\"input_size\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Simulate tool call duration\n",
    "        time.sleep(trace_data[\"tool_call\"][\"duration\"])\n",
    "        \n",
    "        tool_span.log(\n",
    "            output={\"status\": \"completed\"},\n",
    "            metrics={\n",
    "                \"duration\": trace_data[\"tool_call\"][\"duration\"]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # LLM generation span\n",
    "    with start_span(name=\"llm_call\") as llm_span:\n",
    "        llm_data = trace_data[\"llm_call\"]\n",
    "        \n",
    "        llm_span.log(\n",
    "            input=llm_data[\"input\"],\n",
    "            metadata={\n",
    "                \"provider\": llm_data[\"provider\"],\n",
    "                \"model\": llm_data[\"model\"],\n",
    "                \"temperature\": llm_data[\"temperature\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Simulate LLM call duration\n",
    "        time.sleep(llm_data[\"duration\"])\n",
    "        \n",
    "        llm_span.log(\n",
    "            output=llm_data[\"output\"],\n",
    "            metrics={\n",
    "                \"prompt_tokens\": llm_data[\"prompt_tokens\"],\n",
    "                \"completion_tokens\": llm_data[\"completion_tokens\"],\n",
    "                \"total_tokens\": llm_data[\"total_tokens\"],\n",
    "                \"duration\": llm_data[\"duration\"]\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Result span\n",
    "    with start_span(name=\"result\") as result_span:\n",
    "        result_span.log(\n",
    "            input={\"format_requested\": trace_data[\"result\"][\"output_format\"]},\n",
    "            metadata={\n",
    "                \"operation.type\": \"result\",\n",
    "                \"output.format\": trace_data[\"result\"][\"output_format\"]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Simulate result processing duration\n",
    "        time.sleep(trace_data[\"result\"][\"duration\"])\n",
    "        \n",
    "        result_span.log(\n",
    "            output={\"format\": trace_data[\"result\"][\"output_format\"]},\n",
    "            metrics={\n",
    "                \"duration\": trace_data[\"result\"][\"duration\"]\n",
    "            }\n",
    "        )\n",
    "\n",
    "def log_trace_braintrust_wrapper(trace_data):\n",
    "    \"\"\"Wrapper to add user/session context\"\"\"\n",
    "    with start_span(\n",
    "        name=trace_data[\"trace_id\"],\n",
    "        span_attributes={\n",
    "            \"user_id\": trace_data[\"user_id\"],\n",
    "            \"session_id\": trace_data[\"session_id\"]\n",
    "        }\n",
    "    ):\n",
    "        log_trace_braintrust(trace_data)\n",
    "\n",
    "# Run the Braintrust benchmark\n",
    "run_benchmark(\"Braintrust\", log_trace_braintrust_wrapper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
