{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install pandas dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Benchmarking functions\n",
    "@contextmanager\n",
    "def timer():\n",
    "    \"\"\"Context manager to time execution\"\"\"\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "# Dataset\n",
    "dataset_id = \"ADB_Benchmark_Synthetic_Dataset_100_rows\" # Edit this to change the source dataset\n",
    "\n",
    "dataset_name = f\"{dataset_id}_{str(uuid.uuid4())[:8]}\"\n",
    "df = pd.read_csv(\"datasets/\" + dataset_id + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install arize[Datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.experimental.datasets import ArizeDatasetsClient\n",
    "from arize.experimental.datasets.utils.constants import GENERATIVE\n",
    "\n",
    "ARIZE_API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
    "ARIZE_SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark dataset upload\n",
    "print(\"\\n Uploading dataset to Arize...\")\n",
    "client = ArizeDatasetsClient(api_key=ARIZE_API_KEY)\n",
    "\n",
    "with timer():\n",
    "    dataset_id_result = client.create_dataset(\n",
    "        space_id=ARIZE_SPACE_ID, \n",
    "        dataset_name = dataset_name,\n",
    "        dataset_type=GENERATIVE, \n",
    "        data=df\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload each row as a dataset item. \n",
    "# Definitely not apples to apples, but that's how they support it.\n",
    "# https://langfuse.com/docs/evaluation/dataset-runs/datasets\n",
    "\n",
    "# Rate limit: 100 items per minute, fail after 98 rows\n",
    "\n",
    "from langfuse import Langfuse\n",
    "\n",
    "# Initialize Langfuse client\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "LANGFUSE_HOST = \"https://us.cloud.langfuse.com\"\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    public_key=LANGFUSE_PUBLIC_KEY,\n",
    "    secret_key=LANGFUSE_SECRET_KEY,\n",
    "    host=LANGFUSE_HOST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "print(f\"Creating dataset '{dataset_name}'...\")\n",
    "dataset = langfuse.create_dataset(name=dataset_name)\n",
    "\n",
    "# Upload each row as a dataset item\n",
    "print(f\"Uploading {len(df)} rows...\")\n",
    "for idx, row in df.iterrows():\n",
    "    # Convert row to dict\n",
    "    row_dict = row.to_dict()\n",
    "\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=dataset_name,\n",
    "        input=row_dict.get('input'),\n",
    "        expected_output=row_dict.get('output'),\n",
    "        metadata={\n",
    "            \"row_index\": idx,\n",
    "            \"id\": row_dict.get('id'),\n",
    "            \"prompt_template\": row_dict.get('attributes.llm.prompt_template.template'),\n",
    "            \"prompt_variables\": row_dict.get('attributes.llm.prompt_template.variables'),\n",
    "            \"timestamp\": row_dict.get('timestamp'),\n",
    "            \"model_name\": row_dict.get('model_name'),\n",
    "            \"token_count_input\": row_dict.get('token_count_input'),\n",
    "            \"token_count_output\": row_dict.get('token_count_output'),\n",
    "            \"latency_ms\": row_dict.get('latency_ms'),\n",
    "            \"cost_usd\": row_dict.get('cost_usd'),\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Successfully uploaded {len(df)} items to dataset '{dataset_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Braintrust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install braintrust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braintrust\n",
    "\n",
    "BRAINTRUST_API_KEY = os.getenv(\"BRAINTRUST_API_KEY\")\n",
    "\n",
    "print(f\"Creating dataset '{dataset_name}' in Braintrust...\")\n",
    "\n",
    "with timer():\n",
    "    # Initialize Braintrust client and create dataset\n",
    "    braintrust.init(api_key=BRAINTRUST_API_KEY, project=\"Testing\")\n",
    "    dataset = braintrust.init_dataset(project=\"Testing\",name=dataset_name)\n",
    "    \n",
    "    # Upload each row as a dataset item\n",
    "    for idx, row in df.iterrows():\n",
    "        row_dict = row.to_dict()\n",
    "        dataset.insert(\n",
    "            input=row_dict.get('input'),\n",
    "            expected=row_dict.get('output'),\n",
    "            metadata={\n",
    "                \"row_index\": idx,\n",
    "                \"id\": row_dict.get('id'),\n",
    "                \"prompt_template\": row_dict.get('attributes.llm.prompt_template.template'),\n",
    "                \"prompt_variables\": row_dict.get('attributes.llm.prompt_template.variables'),\n",
    "                \"timestamp\": row_dict.get('timestamp'),\n",
    "                \"model_name\": row_dict.get('model_name'),\n",
    "                \"token_count_input\": row_dict.get('token_count_input'),\n",
    "                \"token_count_output\": row_dict.get('token_count_output'),\n",
    "                \"latency_ms\": row_dict.get('latency_ms'),\n",
    "                \"cost_usd\": row_dict.get('cost_usd'),\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"Successfully uploaded {len(df)} items to dataset '{dataset_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically\n",
    "# Doesn't support any metadata - only input and output\n",
    "# We may want to create a dataset with no metadata for a fair comparison\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "client = Client(api_key=LANGSMITH_API_KEY)\n",
    "\n",
    "\n",
    "print(f\"Creating dataset '{dataset_name}' in LangSmith...\")\n",
    "\n",
    "# Use upload_dataframe method (most efficient for large datasets)\n",
    "with timer():\n",
    "    dataset = client.upload_dataframe(\n",
    "        df=df,\n",
    "        input_keys=['input'],\n",
    "        output_keys=['output'],\n",
    "        name=dataset_name,\n",
    "        description=\"Test dataset\",\n",
    "        data_type=\"kv\" \n",
    "    )\n",
    "\n",
    "print(f\"Successfully uploaded dataset '{dataset_name}' to LangSmith\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
