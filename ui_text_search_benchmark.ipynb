{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# UI Text Search Benchmark - Arize\n",
        "\n",
        "This notebook tests the performance of text search functionality in Arize UI by:\n",
        "1. Loading span data (manually downloaded from UI)\n",
        "2. Duplicating rows to reach target number of records\n",
        "3. Modifying input fields to contain 25k characters\n",
        "4. Adding unique searchable keywords to specific rows\n",
        "5. Spreading timestamps over the past 90 days from today\n",
        "6. Uploading to Arize using log_spans method (arize[Tracing])\n",
        "7. Manual testing of search in UI\n",
        "\n",
        "## Prerequisites\n",
        "- Download span data from Arize UI as CSV/DataFrame\n",
        "- Set up environment variables for Arize API credentials\n",
        "\n",
        "## Quick Start\n",
        "1. Download span data from Arize UI (CSV export)\n",
        "2. Update `DATA_FILE_PATH` in cell 4 to point to your downloaded file\n",
        "3. Update `TARGET_ROWS` in cell 2 to set desired dataset size (default: 10M)\n",
        "4. Run all cells to prepare and upload data\n",
        "5. Follow the manual testing instructions at the end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install pandas python-dotenv \"arize[Tracing]\" numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target dataset size: 10,000 rows\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "from contextlib import contextmanager\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Benchmarking functions\n",
        "@contextmanager\n",
        "def timer():\n",
        "    \"\"\"Context manager to time execution\"\"\"\n",
        "    start_time = time.time()\n",
        "    yield\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "    \n",
        "# Configuration\n",
        "TARGET_ROWS = 10_000  # Change this to test different sizes (e.g., 1_000_000, 10_000_000, 20_000_000)\n",
        "TEXT_LENGTH = 25_000  # 25k characters\n",
        "UNIQUE_KEYWORD_ROWS = 10  # Number of rows with unique keywords\n",
        "TIMESTAMP_SPREAD_DAYS = 90  # Spread over past 90 days\n",
        "\n",
        "# Unique keywords for search testing\n",
        "UNIQUE_KEYWORDS = [\n",
        "    \"BENCHMARK_UNIQUE_ALPHA\",\n",
        "    \"BENCHMARK_UNIQUE_BETA\", \n",
        "    \"BENCHMARK_UNIQUE_GAMMA\",\n",
        "    \"BENCHMARK_UNIQUE_DELTA\",\n",
        "    \"BENCHMARK_UNIQUE_EPSILON\",\n",
        "    \"BENCHMARK_UNIQUE_ZETA\",\n",
        "    \"BENCHMARK_UNIQUE_ETA\",\n",
        "    \"BENCHMARK_UNIQUE_THETA\",\n",
        "    \"BENCHMARK_UNIQUE_IOTA\",\n",
        "    \"BENCHMARK_UNIQUE_KAPPA\"\n",
        "]\n",
        "\n",
        "print(f\"Target dataset size: {TARGET_ROWS:,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Downloaded Data\n",
        "\n",
        "First, download span data from Arize UI and save it as a CSV file. Update the path below to point to your downloaded file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 22 rows from datasets/tracing_export.csv\n",
            "Columns: ['name', 'spanKind', 'statusCode', 'start_time', 'parent_id', 'latency_ms', 'context.trace_id', 'context.span_id', 'attributes.session.id', 'attributes.openinference.span.kind', 'totalTokenCount', 'attributes.input.value', 'attributes.output.value', 'attributes.llm.token_count.total', 'attributes.llm.token_count.prompt', ':arize-computed:tokenization.prompt.encoder', ':arize-computed:tokenization.prompt.method', 'attributes.llm.token_count.completion', ':arize-computed:tokenization.completion.encoder', ':arize-computed:tokenization.completion.method', 'status_code']\n"
          ]
        }
      ],
      "source": [
        "# Load the downloaded span data\n",
        "# UPDATE THIS PATH to your downloaded CSV file\n",
        "DATA_FILE_PATH = \"datasets/tracing_export.csv\"  # Change this to your actual file path\n",
        "\n",
        "try:\n",
        "    df_original = pd.read_csv(DATA_FILE_PATH)\n",
        "    print(f\"Loaded {len(df_original)} rows from {DATA_FILE_PATH}\")\n",
        "    print(f\"Columns: {list(df_original.columns)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at {DATA_FILE_PATH}\")\n",
        "    print(\"Please download span data from Arize UI and update the DATA_FILE_PATH variable\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA PREPARATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def generate_large_text(base_text, target_length, unique_keyword=None):\n",
        "    \"\"\"Generate text of specified length with optional unique keyword\"\"\"\n",
        "    # Start with unique keyword if provided\n",
        "    parts = [f\"SEARCHABLE_CONTENT: {unique_keyword}\\n\\n\"] if unique_keyword else []\n",
        "    parts.append(str(base_text) if base_text else \"\")\n",
        "    \n",
        "    # Fill remaining space with lorem ipsum variations\n",
        "    lorem_base = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. \"\n",
        "    while sum(len(p) for p in parts) < target_length:\n",
        "        random_word = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 15)))\n",
        "        parts.append(f\"{lorem_base}Random_{random_word}. \")\n",
        "    \n",
        "    return ''.join(parts)[:target_length]\n",
        "\n",
        "\n",
        "def duplicate_rows(df, target_rows):\n",
        "    \"\"\"Duplicate dataframe rows to reach target number\"\"\"\n",
        "    current_rows = len(df)\n",
        "    if current_rows >= target_rows:\n",
        "        return df\n",
        "    \n",
        "    # Calculate multiplication factor and duplicate\n",
        "    multiplier = (target_rows // current_rows) + 1\n",
        "    print(f\"   Duplicating {current_rows} rows {multiplier}x to reach {target_rows:,}\")\n",
        "    \n",
        "    df_list = [df.copy() for _ in range(multiplier)]\n",
        "    df_final = pd.concat(df_list, ignore_index=True).iloc[:target_rows].copy()\n",
        "    df_final['unique_id'] = [str(uuid.uuid4()) for _ in range(len(df_final))]\n",
        "    \n",
        "    return df_final\n",
        "\n",
        "\n",
        "def spread_timestamps(df, days_back=90):\n",
        "    \"\"\"Spread timestamps over the past N days from today\"\"\"\n",
        "    num_rows = len(df)\n",
        "    end_time = datetime.now()\n",
        "    start_time = end_time - timedelta(days=days_back)\n",
        "    \n",
        "    print(f\"   Spreading {num_rows:,} timestamps from {start_time.strftime('%Y-%m-%d')} to {end_time.strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    # Generate evenly spaced timestamps and shuffle them\n",
        "    time_increment = timedelta(days=days_back) / num_rows\n",
        "    timestamps = [start_time + (i * time_increment) for i in range(num_rows)]\n",
        "    random.shuffle(timestamps)\n",
        "    \n",
        "    # Update timestamp column (prefer start_time for log_spans compatibility)\n",
        "    timestamp_col = next((col for col in ['start_time', 'timestamp', 'time'] if col in df.columns), 'start_time')\n",
        "    df[timestamp_col] = timestamps\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_data_simplified(df_original, target_rows, text_length=TEXT_LENGTH):\n",
        "    \"\"\"Simplified data preparation pipeline\"\"\"\n",
        "    print(f\"\\n=== Preparing {target_rows:,} rows ===\")\n",
        "    \n",
        "    # Step 1: Duplicate to target size\n",
        "    print(\"1. Duplicating rows...\")\n",
        "    with timer():\n",
        "        df = duplicate_rows(df_original, target_rows)\n",
        "    \n",
        "    # Step 2: Spread timestamps over past 90 days\n",
        "    print(\"2. Spreading timestamps...\")\n",
        "    with timer():\n",
        "        df = spread_timestamps(df, TIMESTAMP_SPREAD_DAYS)\n",
        "    \n",
        "    # Step 3: Generate large text with keywords\n",
        "    print(f\"3. Generating {text_length:,}-char text...\")\n",
        "    with timer():\n",
        "        input_col = next((col for col in ['attributes.input.value', 'input', 'prompt'] if col in df.columns), 'attributes.input.value')\n",
        "        \n",
        "        # Add keywords to first 10 rows for search testing\n",
        "        for i in range(len(df)):\n",
        "            keyword = UNIQUE_KEYWORDS[i] if i < min(len(UNIQUE_KEYWORDS), len(df)) else None\n",
        "            base_text = df.iloc[i].get(input_col, \"\")\n",
        "            df.iloc[i, df.columns.get_loc(input_col)] = generate_large_text(base_text, text_length, keyword)\n",
        "            \n",
        "            if keyword:\n",
        "                print(f\"   Row {i}: '{keyword}'\")\n",
        "    \n",
        "    print(f\"âœ… Ready: {len(df):,} rows\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Preparation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_benchmark_data(df_original, target_rows, text_length=TEXT_LENGTH):\n",
        "    \"\"\"Prepare data for benchmark with all transformations\"\"\"\n",
        "    \n",
        "    print(f\"\\n=== Preparing data for {target_rows:,} rows ===\")\n",
        "    \n",
        "    # Step 1: Duplicate rows\n",
        "    print(\"\\n1. Duplicating rows...\")\n",
        "    with timer():\n",
        "        df = duplicate_rows(df_original, target_rows)\n",
        "    print(f\"   Result: {len(df):,} rows\")\n",
        "    \n",
        "    # Step 2: Spread timestamps over past 90 days\n",
        "    with timer():\n",
        "        df = spread_timestamps(df, TIMESTAMP_SPREAD_DAYS)\n",
        "    \n",
        "    # Verify timestamp spreading\n",
        "    if 'start_time' in df.columns:\n",
        "        print(f\"   Timestamp range: {df['start_time'].min()} to {df['start_time'].max()}\")\n",
        "    elif 'timestamp' in df.columns:\n",
        "        print(f\"   Timestamp range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
        "    \n",
        "    # Step 3: Generate large text for input fields\n",
        "    print(f\"\\n3. Generating {text_length:,} character text for input fields...\")\n",
        "    print(\"   Adding unique keywords to search test rows...\")\n",
        "    \n",
        "    # Find the input column (might be named differently)\n",
        "    input_col = None\n",
        "    for col in ['input', 'input.value', 'attributes.input.value', 'prompt']:\n",
        "        if col in df.columns:\n",
        "            input_col = col\n",
        "            break\n",
        "    \n",
        "    if input_col is None:\n",
        "        print(\"   WARNING: No input column found. Creating new 'input' column.\")\n",
        "        df['input'] = \"\"\n",
        "        input_col = 'input'\n",
        "    \n",
        "    # Select random rows for unique keywords and create mapping\n",
        "    keyword_sample = random.sample(range(len(df)), min(UNIQUE_KEYWORD_ROWS, len(df)))\n",
        "    keyword_mapping = {idx: UNIQUE_KEYWORDS[i] for i, idx in enumerate(keyword_sample)}\n",
        "    \n",
        "    print(f\"   Keywords will be placed at rows: {sorted(keyword_mapping.keys())[:5]}...\")  # Show first 5\n",
        "    \n",
        "    # Generate base text pattern once\n",
        "    lorem_variations = [\n",
        "        \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. \",\n",
        "        \"Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. \",\n",
        "        \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris. \",\n",
        "        \"Duis aute irure dolor in reprehenderit in voluptate velit esse cillum. \",\n",
        "        \"Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia. \",\n",
        "    ]\n",
        "    \n",
        "    # Create a large text block that we can reuse\n",
        "    print(\"   Generating base text block...\")\n",
        "    base_text_block = \"\"\n",
        "    while len(base_text_block) < text_length + 1000:  # Extra buffer\n",
        "        random_word = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 15)))\n",
        "        base_text_block += random.choice(lorem_variations) + f\"Random_{random_word}. \"\n",
        "    \n",
        "    # Use numpy for efficient array operations\n",
        "    print(\"   Creating text array...\")\n",
        "    text_array = np.empty(len(df), dtype=object)\n",
        "    \n",
        "    # Fill with base text\n",
        "    text_array[:] = base_text_block[:text_length]\n",
        "    \n",
        "    # Add keywords to specific positions\n",
        "    for idx, keyword in keyword_mapping.items():\n",
        "        keyword_text = f\"SEARCHABLE_CONTENT: {keyword}\\n\\n\" + base_text_block[:text_length - len(f\"SEARCHABLE_CONTENT: {keyword}\\n\\n\")]\n",
        "        text_array[idx] = keyword_text\n",
        "        print(f\"   Added keyword '{keyword}' to row {idx}\")\n",
        "    \n",
        "    # Assign all at once\n",
        "    print(\"   Assigning text to dataframe...\")\n",
        "    with timer():\n",
        "        df[input_col] = text_array\n",
        "    \n",
        "    print(f\"\\n4. Data preparation complete!\")\n",
        "    print(f\"   Total rows: {len(df):,}\")\n",
        "    print(f\"   Unique keywords added to {len(keyword_mapping)} rows\")\n",
        "    print(f\"   Keywords used: {', '.join(UNIQUE_KEYWORDS[:len(keyword_mapping)])}\")\n",
        "    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Preparing 10,000 rows ===\n",
            "1. Duplicating rows...\n",
            "   Duplicating 22 rows 455x to reach 10,000\n",
            "Execution time: 0.05 seconds\n",
            "2. Spreading timestamps...\n",
            "   Spreading 10,000 timestamps from 2025-06-03 to 2025-09-01\n",
            "Execution time: 0.01 seconds\n",
            "3. Generating 25,000-char text...\n",
            "   Row 0: 'BENCHMARK_UNIQUE_ALPHA'\n",
            "   Row 1: 'BENCHMARK_UNIQUE_BETA'\n",
            "   Row 2: 'BENCHMARK_UNIQUE_GAMMA'\n",
            "   Row 3: 'BENCHMARK_UNIQUE_DELTA'\n",
            "   Row 4: 'BENCHMARK_UNIQUE_EPSILON'\n",
            "   Row 5: 'BENCHMARK_UNIQUE_ZETA'\n",
            "   Row 6: 'BENCHMARK_UNIQUE_ETA'\n",
            "   Row 7: 'BENCHMARK_UNIQUE_THETA'\n",
            "   Row 8: 'BENCHMARK_UNIQUE_IOTA'\n",
            "   Row 9: 'BENCHMARK_UNIQUE_KAPPA'\n",
            "Execution time: 16.42 seconds\n",
            "âœ… Ready: 10,000 rows\n"
          ]
        }
      ],
      "source": [
        "# Prepare dataset with target number of rows\n",
        "df_prepared = prepare_data_simplified(df_original, TARGET_ROWS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Setup Arize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  arize.utils.logging | WARNING | The developer_key parameter is deprecated and will be removed in a future release. You only need the api_key for all data logging operations.\u001b[0m\n",
            "Arize project name: TextSearchBench-10000-1434\n",
            "âœ… Arize client setup complete!\n"
          ]
        }
      ],
      "source": [
        "from arize.pandas.logger import Client\n",
        "\n",
        "# Configuration for span logging\n",
        "ARIZE_SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\")\n",
        "ARIZE_API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
        "\n",
        "# Create a unique project name for this benchmark\n",
        "PROJECT_NAME = f\"TextSearchBench-{TARGET_ROWS}-{datetime.now().strftime('%H%M')}\"\n",
        "\n",
        "# Setup Arize client for logging spans\n",
        "arize_client = Client(\n",
        "    space_id=ARIZE_SPACE_ID,\n",
        "    api_key=ARIZE_API_KEY,\n",
        ")\n",
        "\n",
        "print(f\"Arize project name: {PROJECT_NAME}\")\n",
        "print(\"âœ… Arize client setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Upload Spans to Arize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_spans_to_arize(df):\n",
        "    \"\"\"Upload dataframe rows as spans to Arize using log_spans\"\"\"\n",
        "    \n",
        "    total_rows = len(df)\n",
        "    print(f\"\\n=== Uploading {total_rows:,} spans to Arize using log_spans ===\")\n",
        "    print(f\"Project: {PROJECT_NAME}\")\n",
        "    \n",
        "    print(\"\\nPreparing spans DataFrame for log_spans...\")\n",
        "    print(\"Using known column structure from tracing_export.csv format\")\n",
        "    \n",
        "    with timer():\n",
        "        # Create clean spans DataFrame with only the required columns\n",
        "        spans_df = pd.DataFrame()\n",
        "        \n",
        "        # Required columns - ensure proper data types\n",
        "        spans_df['context.trace_id'] = df['context.trace_id'].fillna('').astype(str)\n",
        "        spans_df['context.span_id'] = df['context.span_id'].fillna('').astype(str)\n",
        "        spans_df['name'] = df['name'].fillna('LLM_span').astype(str)\n",
        "        \n",
        "        # Handle timestamps\n",
        "        spans_df['start_time'] = pd.to_datetime(df['start_time'], errors='coerce')\n",
        "        \n",
        "        # Calculate end times using latency_ms\n",
        "        if 'latency_ms' in df.columns:\n",
        "            # Ensure latency_ms is numeric\n",
        "            latency_ms = pd.to_numeric(df['latency_ms'], errors='coerce').fillna(1000.0)\n",
        "            spans_df['end_time'] = spans_df['start_time'] + pd.to_timedelta(latency_ms, unit='ms')\n",
        "        else:\n",
        "            # Generate random latency if not available\n",
        "            latency_ms = [random.uniform(100, 2000) for _ in range(len(df))]\n",
        "            spans_df['end_time'] = spans_df['start_time'] + pd.to_timedelta(latency_ms, unit='ms')\n",
        "        \n",
        "        # Handle input/output attributes - ensure they are strings\n",
        "        if 'attributes.input.value' in df.columns:\n",
        "            spans_df['attributes.input.value'] = df['attributes.input.value'].fillna('').astype(str)\n",
        "        \n",
        "        if 'attributes.output.value' in df.columns:\n",
        "            spans_df['attributes.output.value'] = df['attributes.output.value'].fillna('').astype(str)\n",
        "        \n",
        "        # Handle status code\n",
        "        if 'status_code' in df.columns:\n",
        "            spans_df['status_code'] = df['status_code'].fillna('OK').astype(str)\n",
        "        else:\n",
        "            spans_df['status_code'] = 'OK'\n",
        "        \n",
        "        # Handle span kind\n",
        "        if 'attributes.openinference.span.kind' in df.columns:\n",
        "            spans_df['attributes.openinference.span.kind'] = df['attributes.openinference.span.kind'].fillna('LLM').astype(str)\n",
        "        else:\n",
        "            spans_df['attributes.openinference.span.kind'] = 'LLM'\n",
        "        \n",
        "        # Handle parent_id if present\n",
        "        if 'parent_id' in df.columns:\n",
        "            # Only include non-null parent_ids\n",
        "            parent_ids = df['parent_id'].fillna('')\n",
        "            spans_df['parent_id'] = parent_ids.astype(str)\n",
        "            # Replace empty strings with None for proper parent relationship\n",
        "            spans_df.loc[spans_df['parent_id'] == '', 'parent_id'] = None\n",
        "        \n",
        "        # Handle token counts if present (ensure they are numeric)\n",
        "        if 'totalTokenCount' in df.columns:\n",
        "            spans_df['attributes.llm.token_count.total'] = pd.to_numeric(df['totalTokenCount'], errors='coerce').fillna(0).astype(int)\n",
        "        \n",
        "        if 'attributes.llm.token_count.prompt' in df.columns:\n",
        "            spans_df['attributes.llm.token_count.prompt'] = pd.to_numeric(df['attributes.llm.token_count.prompt'], errors='coerce').fillna(0).astype(int)\n",
        "        \n",
        "        if 'attributes.llm.token_count.completion' in df.columns:\n",
        "            spans_df['attributes.llm.token_count.completion'] = pd.to_numeric(df['attributes.llm.token_count.completion'], errors='coerce').fillna(0).astype(int)\n",
        "        \n",
        "        # Add unique_id for tracking (from our benchmark preparation)\n",
        "        if 'unique_id' in df.columns:\n",
        "            spans_df['unique_id'] = df['unique_id'].astype(str)\n",
        "        else:\n",
        "            spans_df['unique_id'] = [str(uuid.uuid4()) for _ in range(len(spans_df))]\n",
        "        \n",
        "        print(f\"   Prepared spans DataFrame with {len(spans_df)} rows and {len(spans_df.columns)} columns\")\n",
        "        print(f\"   Key columns: {[col for col in ['context.trace_id', 'context.span_id', 'name', 'start_time', 'end_time', 'attributes.input.value'] if col in spans_df.columns]}\")\n",
        "        \n",
        "        # Upload to Arize using log_spans\n",
        "        print(\"\\nðŸš€ Uploading spans to Arize...\")\n",
        "        response = arize_client.log_spans(\n",
        "            dataframe=spans_df,\n",
        "            model_id=PROJECT_NAME,\n",
        "            model_version=\"1.0\",\n",
        "            validate=True,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        # Check response\n",
        "        if response.status_code == 200:\n",
        "            print(f\"âœ… Successfully uploaded {total_rows:,} spans to Arize!\")\n",
        "            print(f\"   Project: {PROJECT_NAME}\")\n",
        "            print(f\"   Response: {response.status_code}\")\n",
        "        else:\n",
        "            print(f\"âŒ Upload failed with status code: {response.status_code}\")\n",
        "            print(f\"   Response text: {response.text}\")\n",
        "            \n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Uploading 10,000 spans to Arize using log_spans ===\n",
            "Project: TextSearchBench-10000-1434\n",
            "\n",
            "Preparing spans DataFrame for log_spans...\n",
            "Using known column structure from tracing_export.csv format\n",
            "   Prepared spans DataFrame with 10000 rows and 14 columns\n",
            "   Key columns: ['context.trace_id', 'context.span_id', 'name', 'start_time', 'end_time', 'attributes.input.value']\n",
            "\n",
            "ðŸš€ Uploading spans to Arize...\n",
            "\u001b[38;21m  arize.utils.logging | INFO | Performing direct input type validation.\u001b[0m\n",
            "\u001b[38;21m  arize.utils.logging | INFO | Performing dataframe form validation.\u001b[0m\n",
            "\u001b[38;21m  arize.utils.logging | INFO | The following columns are not part of the Open Inference Specification and will be ignored: unique_id\u001b[0m\n",
            "\u001b[38;21m  arize.utils.logging | INFO | Performing values validation.\u001b[0m\n",
            "\u001b[38;21m  arize.utils.logging | INFO | Sending file to Arize\u001b[0m\n",
            "\u001b[38;21m  arize.utils.logging | INFO | Success! Check out your data at https://app.arize.com/organizations/QWNjb3VudE9yZ2FuaXphdGlvbjoyMzczNjpLYVg4/spaces/U3BhY2U6MjQ3NzI6YUlOdg==/models/modelName/TextSearchBench-10000-1434?selectedTab=llmTracing\u001b[0m\n",
            "âœ… Successfully uploaded 10,000 spans to Arize!\n",
            "   Project: TextSearchBench-10000-1434\n",
            "   Response: 200\n",
            "Execution time: 16.04 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Upload the prepared dataset to Arize\n",
        "upload_spans_to_arize(df_prepared)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
