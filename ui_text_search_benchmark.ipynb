{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install pandas python-dotenv \"arize[Tracing]\" numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "from contextlib import contextmanager\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Benchmarking functions\n",
        "@contextmanager\n",
        "def timer():\n",
        "    \"\"\"Context manager to time execution\"\"\"\n",
        "    start_time = time.time()\n",
        "    yield\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "    \n",
        "# Configuration\n",
        "TARGET_ROWS = 10_000_000  # Change this to test different sizes (e.g., 1_000_000, 10_000_000, 20_000_000)\n",
        "TEXT_LENGTH = 25_000  # 25k characters\n",
        "UNIQUE_KEYWORD_ROWS = 10000  # Number of rows with unique keywords\n",
        "TIMESTAMP_SPREAD_DAYS = 90  # Spread over past 90 days\n",
        "\n",
        "# Unique keywords for search testing\n",
        "UNIQUE_KEYWORDS = [\n",
        "    \"BENCHMARK_UNIQUE_ALPHA\",\n",
        "    \"BENCHMARK_UNIQUE_BETA\", \n",
        "    \"BENCHMARK_UNIQUE_GAMMA\",\n",
        "    \"BENCHMARK_UNIQUE_DELTA\",\n",
        "    \"BENCHMARK_UNIQUE_EPSILON\",\n",
        "    \"BENCHMARK_UNIQUE_ZETA\",\n",
        "    \"BENCHMARK_UNIQUE_ETA\",\n",
        "    \"BENCHMARK_UNIQUE_THETA\",\n",
        "    \"BENCHMARK_UNIQUE_IOTA\",\n",
        "    \"BENCHMARK_UNIQUE_KAPPA\"\n",
        "]\n",
        "\n",
        "print(f\"Target dataset size: {TARGET_ROWS:,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Downloaded Data\n",
        "\n",
        "First, download span data from Arize UI and save it as a CSV file. Update the path below to point to your downloaded file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the downloaded span data\n",
        "# UPDATE THIS PATH to your downloaded CSV file\n",
        "DATA_FILE_PATH = \"tracing_export.csv\"  # Change this to your actual file path\n",
        "\n",
        "try:\n",
        "    df_original = pd.read_csv(DATA_FILE_PATH)\n",
        "    print(f\"Loaded {len(df_original)} rows from {DATA_FILE_PATH}\")\n",
        "    print(f\"Columns: {list(df_original.columns)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at {DATA_FILE_PATH}\")\n",
        "    print(\"Please download span data from Arize UI and update the DATA_FILE_PATH variable\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA PREPARATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def generate_large_text(base_text, target_length, unique_keyword=None):\n",
        "    \"\"\"Generate text of specified length with optional unique keyword\"\"\"\n",
        "    # Start with unique keyword if provided\n",
        "    parts = [f\"SEARCHABLE_CONTENT: {unique_keyword}\\n\\n\"] if unique_keyword else []\n",
        "    parts.append(str(base_text) if base_text else \"\")\n",
        "    \n",
        "    # Fill remaining space with lorem ipsum variations\n",
        "    lorem_base = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. \"\n",
        "    while sum(len(p) for p in parts) < target_length:\n",
        "        random_word = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 15)))\n",
        "        parts.append(f\"{lorem_base}Random_{random_word}. \")\n",
        "    \n",
        "    return ''.join(parts)[:target_length]\n",
        "\n",
        "\n",
        "def duplicate_rows(df, target_rows):\n",
        "    \"\"\"Duplicate dataframe rows to reach target number\"\"\"\n",
        "    current_rows = len(df)\n",
        "    if current_rows >= target_rows:\n",
        "        return df\n",
        "    \n",
        "    # Calculate multiplication factor and duplicate\n",
        "    multiplier = (target_rows // current_rows) + 1\n",
        "    print(f\"   Duplicating {current_rows} rows {multiplier}x to reach {target_rows:,}\")\n",
        "    \n",
        "    df_list = [df.copy() for _ in range(multiplier)]\n",
        "    df_final = pd.concat(df_list, ignore_index=True).iloc[:target_rows].copy()\n",
        "    df_final['unique_id'] = [str(uuid.uuid4()) for _ in range(len(df_final))]\n",
        "    \n",
        "    return df_final\n",
        "\n",
        "\n",
        "def spread_timestamps(df, days_back=90):\n",
        "    \"\"\"Spread timestamps over the past N days from today\"\"\"\n",
        "    num_rows = len(df)\n",
        "    end_time = datetime.now()\n",
        "    start_time = end_time - timedelta(days=days_back)\n",
        "    \n",
        "    print(f\"   Spreading {num_rows:,} timestamps from {start_time.strftime('%Y-%m-%d')} to {end_time.strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    # Generate evenly spaced timestamps and shuffle them\n",
        "    time_increment = timedelta(days=days_back) / num_rows\n",
        "    timestamps = [start_time + (i * time_increment) for i in range(num_rows)]\n",
        "    random.shuffle(timestamps)\n",
        "    \n",
        "    # Update timestamp column (prefer start_time for log_spans compatibility)\n",
        "    timestamp_col = next((col for col in ['start_time', 'timestamp', 'time'] if col in df.columns), 'start_time')\n",
        "    df[timestamp_col] = timestamps\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_data_simplified(df_original, target_rows, text_length=TEXT_LENGTH):\n",
        "    \"\"\"Simplified data preparation pipeline\"\"\"\n",
        "    print(f\"\\n=== Preparing {target_rows:,} rows ===\")\n",
        "    \n",
        "    # Step 1: Duplicate to target size\n",
        "    print(\"1. Duplicating rows...\")\n",
        "    with timer():\n",
        "        df = duplicate_rows(df_original, target_rows)\n",
        "    \n",
        "    # Step 2: Spread timestamps over past 90 days\n",
        "    print(\"2. Spreading timestamps...\")\n",
        "    with timer():\n",
        "        df = spread_timestamps(df, TIMESTAMP_SPREAD_DAYS)\n",
        "    \n",
        "    # Step 3: Generate large text with keywords\n",
        "    print(f\"3. Generating {text_length:,}-char text...\")\n",
        "    with timer():\n",
        "        input_col = next((col for col in ['attributes.input.value', 'input', 'prompt'] if col in df.columns), 'attributes.input.value')\n",
        "        \n",
        "        # Add keywords to first 10 rows for search testing\n",
        "        for i in range(len(df)):\n",
        "            keyword = UNIQUE_KEYWORDS[i] if i < min(len(UNIQUE_KEYWORDS), len(df)) else None\n",
        "            base_text = df.iloc[i].get(input_col, \"\")\n",
        "            df.iloc[i, df.columns.get_loc(input_col)] = generate_large_text(base_text, text_length, keyword)\n",
        "            \n",
        "            if keyword:\n",
        "                print(f\"   Row {i}: '{keyword}'\")\n",
        "    \n",
        "    print(f\"âœ… Ready: {len(df):,} rows\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset with target number of rows\n",
        "df_prepared = prepare_data_simplified(df_original, TARGET_ROWS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Spans to Arize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from arize.pandas.logger import Client\n",
        "\n",
        "# Configuration for span logging\n",
        "ARIZE_SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\")\n",
        "ARIZE_API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
        "\n",
        "# Create a unique project name for this benchmark\n",
        "PROJECT_NAME = f\"TextSearchBench-{TARGET_ROWS}-{datetime.now().strftime('%H%M')}\"\n",
        "\n",
        "# Setup Arize client for logging spans\n",
        "arize_client = Client(\n",
        "    space_id=ARIZE_SPACE_ID,\n",
        "    api_key=ARIZE_API_KEY,\n",
        ")\n",
        "\n",
        "print(f\"Arize project name: {PROJECT_NAME}\")\n",
        "print(\"âœ… Arize client setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_spans_to_arize(df):\n",
        "    \"\"\"Upload dataframe rows as spans to Arize using log_spans\"\"\"\n",
        "    \n",
        "    total_rows = len(df)\n",
        "    print(f\"\\n=== Uploading {total_rows:,} spans to Arize using log_spans ===\")\n",
        "    print(f\"Project: {PROJECT_NAME}\")\n",
        "    \n",
        "    print(\"\\nPreparing spans DataFrame for log_spans...\")\n",
        "    print(\"Using known column structure from tracing_export.csv format\")\n",
        "    \n",
        "    with timer():\n",
        "        # Create clean spans DataFrame with only the required columns\n",
        "        spans_df = pd.DataFrame()\n",
        "        \n",
        "        # Required columns - ensure proper data types\n",
        "        spans_df['context.trace_id'] = df['context.trace_id'].fillna('').astype(str)\n",
        "        spans_df['context.span_id'] = df['context.span_id'].fillna('').astype(str)\n",
        "        spans_df['name'] = df['name'].fillna('LLM_span').astype(str)\n",
        "        \n",
        "        # Handle timestamps\n",
        "        spans_df['start_time'] = pd.to_datetime(df['start_time'], errors='coerce')\n",
        "        \n",
        "        # Calculate end times using latency_ms\n",
        "        if 'latency_ms' in df.columns:\n",
        "            # Ensure latency_ms is numeric\n",
        "            latency_ms = pd.to_numeric(df['latency_ms'], errors='coerce').fillna(1000.0)\n",
        "            spans_df['end_time'] = spans_df['start_time'] + pd.to_timedelta(latency_ms, unit='ms')\n",
        "        else:\n",
        "            # Generate random latency if not available\n",
        "            latency_ms = [random.uniform(100, 2000) for _ in range(len(df))]\n",
        "            spans_df['end_time'] = spans_df['start_time'] + pd.to_timedelta(latency_ms, unit='ms')\n",
        "        \n",
        "        # Handle input/output attributes - ensure they are strings\n",
        "        if 'attributes.input.value' in df.columns:\n",
        "            spans_df['attributes.input.value'] = df['attributes.input.value'].fillna('').astype(str)\n",
        "        \n",
        "        if 'attributes.output.value' in df.columns:\n",
        "            spans_df['attributes.output.value'] = df['attributes.output.value'].fillna('').astype(str)\n",
        "        \n",
        "        # Handle status code\n",
        "        if 'status_code' in df.columns:\n",
        "            spans_df['status_code'] = df['status_code'].fillna('OK').astype(str)\n",
        "        else:\n",
        "            spans_df['status_code'] = 'OK'\n",
        "        \n",
        "        # Handle span kind\n",
        "        if 'attributes.openinference.span.kind' in df.columns:\n",
        "            spans_df['attributes.openinference.span.kind'] = df['attributes.openinference.span.kind'].fillna('LLM').astype(str)\n",
        "        else:\n",
        "            spans_df['attributes.openinference.span.kind'] = 'LLM'\n",
        "        \n",
        "        # Handle parent_id if present\n",
        "        if 'parent_id' in df.columns:\n",
        "            # Only include non-null parent_ids\n",
        "            parent_ids = df['parent_id'].fillna('')\n",
        "            spans_df['parent_id'] = parent_ids.astype(str)\n",
        "            # Replace empty strings with None for proper parent relationship\n",
        "            spans_df.loc[spans_df['parent_id'] == '', 'parent_id'] = None\n",
        "        \n",
        "        # Handle token counts if present (ensure they are numeric)\n",
        "        if 'totalTokenCount' in df.columns:\n",
        "            spans_df['attributes.llm.token_count.total'] = pd.to_numeric(df['totalTokenCount'], errors='coerce').fillna(0).astype(int)\n",
        "        \n",
        "        if 'attributes.llm.token_count.prompt' in df.columns:\n",
        "            spans_df['attributes.llm.token_count.prompt'] = pd.to_numeric(df['attributes.llm.token_count.prompt'], errors='coerce').fillna(0).astype(int)\n",
        "        \n",
        "        if 'attributes.llm.token_count.completion' in df.columns:\n",
        "            spans_df['attributes.llm.token_count.completion'] = pd.to_numeric(df['attributes.llm.token_count.completion'], errors='coerce').fillna(0).astype(int)\n",
        "        \n",
        "        # Add unique_id for tracking (from our benchmark preparation)\n",
        "        if 'unique_id' in df.columns:\n",
        "            spans_df['unique_id'] = df['unique_id'].astype(str)\n",
        "        else:\n",
        "            spans_df['unique_id'] = [str(uuid.uuid4()) for _ in range(len(spans_df))]\n",
        "        \n",
        "        print(f\"   Prepared spans DataFrame with {len(spans_df)} rows and {len(spans_df.columns)} columns\")\n",
        "        print(f\"   Key columns: {[col for col in ['context.trace_id', 'context.span_id', 'name', 'start_time', 'end_time', 'attributes.input.value'] if col in spans_df.columns]}\")\n",
        "        \n",
        "        # Upload to Arize using log_spans\n",
        "        print(\"\\nðŸš€ Uploading spans to Arize...\")\n",
        "        response = arize_client.log_spans(\n",
        "            dataframe=spans_df,\n",
        "            model_id=PROJECT_NAME,\n",
        "            model_version=\"1.0\",\n",
        "            validate=True,\n",
        "            verbose=True\n",
        "        )\n",
        "        \n",
        "        # Check response\n",
        "        if response.status_code == 200:\n",
        "            print(f\"âœ… Successfully uploaded {total_rows:,} spans to Arize!\")\n",
        "            print(f\"   Project: {PROJECT_NAME}\")\n",
        "            print(f\"   Response: {response.status_code}\")\n",
        "        else:\n",
        "            print(f\"âŒ Upload failed with status code: {response.status_code}\")\n",
        "            print(f\"   Response text: {response.text}\")\n",
        "            \n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the prepared dataset to Arize\n",
        "upload_spans_to_arize(df_prepared)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
