{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!uv pip install pandas python-dotenv \"arize[Tracing]\" numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "from contextlib import contextmanager\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Benchmarking functions\n",
        "@contextmanager\n",
        "def timer():\n",
        "    \"\"\"Context manager to time execution\"\"\"\n",
        "    start_time = time.time()\n",
        "    yield\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    print(f\"Execution time: {execution_time:.2f} seconds\")\n",
        "    \n",
        "# Configuration\n",
        "TARGET_ROWS = 10_000_000  # Change this to test different sizes (e.g., 1_000_000, 10_000_000, 20_000_000)\n",
        "TEXT_LENGTH = 25_000  # 25k characters\n",
        "UNIQUE_KEYWORD_ROWS = 10_000  # Number of rows with unique keywords\n",
        "TIMESTAMP_SPREAD_DAYS = 90  # Spread over past 90 days\n",
        "\n",
        "# Unique keywords for search testing\n",
        "UNIQUE_KEYWORDS = [\n",
        "    \"BENCHMARK_ALPHA\",\n",
        "    \"BENCHMARK_BETA\", \n",
        "    \"BENCHMARK_GAMMA\",\n",
        "    \"BENCHMARK_DELTA\",\n",
        "    \"BENCHMARK_EPSILON\",\n",
        "    \"BENCHMARK_ZETA\",\n",
        "    \"BENCHMARK_ETA\",\n",
        "    \"BENCHMARK_THETA\",\n",
        "    \"BENCHMARK_IOTA\",\n",
        "    \"BENCHMARK_KAPPA\"\n",
        "]\n",
        "\n",
        "print(f\"Target dataset size: {TARGET_ROWS:,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Downloaded Data\n",
        "\n",
        "First, download span data from Arize UI and save it as a CSV file. Update the path below to point to your downloaded file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the downloaded span data\n",
        "# UPDATE THIS PATH to your downloaded CSV file\n",
        "DATA_FILE_PATH = \"tracing_export.csv\"  # Change this to your actual file path\n",
        "\n",
        "try:\n",
        "    df_original = pd.read_csv(DATA_FILE_PATH)\n",
        "    print(f\"Loaded {len(df_original)} rows from {DATA_FILE_PATH}\")\n",
        "    print(f\"Columns: {list(df_original.columns)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: File not found at {DATA_FILE_PATH}\")\n",
        "    print(\"Please download span data from Arize UI and update the DATA_FILE_PATH variable\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# DATA PREPARATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def generate_large_text(base_text, target_length, unique_keyword=None):\n",
        "    \"\"\"Generate text of specified length with optional unique keyword\"\"\"\n",
        "    # Start with unique keyword if provided\n",
        "    parts = [f\"SEARCHABLE_CONTENT: {unique_keyword}\\n\\n\"] if unique_keyword else []\n",
        "    parts.append(str(base_text) if base_text else \"\")\n",
        "    \n",
        "    # Fill remaining space with lorem ipsum variations\n",
        "    lorem_base = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit. \"\n",
        "    while sum(len(p) for p in parts) < target_length:\n",
        "        random_word = ''.join(random.choices(string.ascii_lowercase, k=random.randint(5, 15)))\n",
        "        parts.append(f\"{lorem_base}Random_{random_word}. \")\n",
        "    \n",
        "    return ''.join(parts)[:target_length]\n",
        "\n",
        "\n",
        "def duplicate_rows(df, target_rows):\n",
        "    \"\"\"Duplicate dataframe rows to reach target number\"\"\"\n",
        "    current_rows = len(df)\n",
        "    if current_rows >= target_rows:\n",
        "        return df\n",
        "    \n",
        "    # Calculate multiplication factor and duplicate\n",
        "    multiplier = (target_rows // current_rows) + 1\n",
        "    print(f\"   Duplicating {current_rows} rows {multiplier}x to reach {target_rows:,}\")\n",
        "    \n",
        "    df_list = [df.copy() for _ in range(multiplier)]\n",
        "    df_final = pd.concat(df_list, ignore_index=True).iloc[:target_rows].copy()\n",
        "    df_final['unique_id'] = [str(uuid.uuid4()) for _ in range(len(df_final))]\n",
        "    \n",
        "    return df_final\n",
        "\n",
        "\n",
        "def spread_timestamps(df, days_back=90):\n",
        "    \"\"\"Spread timestamps over the past N days from today\"\"\"\n",
        "    num_rows = len(df)\n",
        "    end_time = datetime.now()\n",
        "    start_time = end_time - timedelta(days=days_back)\n",
        "    \n",
        "    print(f\"   Spreading {num_rows:,} timestamps from {start_time.strftime('%Y-%m-%d')} to {end_time.strftime('%Y-%m-%d')}\")\n",
        "    \n",
        "    # Generate evenly spaced timestamps and shuffle them\n",
        "    time_increment = timedelta(days=days_back) / num_rows\n",
        "    timestamps = [start_time + (i * time_increment) for i in range(num_rows)]\n",
        "    random.shuffle(timestamps)\n",
        "    \n",
        "    # Update timestamp column (prefer start_time for log_spans compatibility)\n",
        "    timestamp_col = next((col for col in ['start_time', 'timestamp', 'time'] if col in df.columns), 'start_time')\n",
        "    df[timestamp_col] = timestamps\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_data(df_original, target_rows, text_length=TEXT_LENGTH):\n",
        "    \"\"\"Optimized data preparation pipeline for any dataset size\"\"\"\n",
        "    print(f\"\\n=== Preparing {target_rows:,} rows ===\")\n",
        "    \n",
        "    # Step 1: Duplicate to target size\n",
        "    print(\"1. Duplicating rows...\")\n",
        "    with timer():\n",
        "        df = duplicate_rows(df_original, target_rows)\n",
        "    \n",
        "    # Step 2: Spread timestamps over past 90 days\n",
        "    print(\"2. Spreading timestamps...\")\n",
        "    with timer():\n",
        "        df = spread_timestamps(df, TIMESTAMP_SPREAD_DAYS)\n",
        "    \n",
        "    # Step 3: Generate large text with keywords (universally optimized)\n",
        "    print(f\"3. Generating {text_length:,}-char text...\")\n",
        "    with timer():\n",
        "        input_col = next((col for col in ['attributes.input.value', 'input', 'prompt'] if col in df.columns), 'attributes.input.value')\n",
        "        \n",
        "        # Random keyword distribution\n",
        "        keyword_rows = min(UNIQUE_KEYWORD_ROWS, len(df))\n",
        "        print(f\"   Distributing {keyword_rows} keywords across {len(df):,} rows...\")\n",
        "        \n",
        "        # Efficient random sampling (works for all dataset sizes)\n",
        "        if len(df) > 1_000_000:\n",
        "            import numpy as np\n",
        "            random_indices = np.random.choice(len(df), size=keyword_rows, replace=False)\n",
        "            random_keyword_indices = set(random_indices)\n",
        "        else:\n",
        "            random_keyword_indices = set(random.sample(range(len(df)), keyword_rows))\n",
        "        \n",
        "        # Pre-generate all text variants once (major performance boost)\n",
        "        print(\"   Pre-generating text templates...\")\n",
        "        base_text = generate_large_text(\"\", text_length, None)\n",
        "        keyword_texts = {kw: generate_large_text(\"\", text_length, kw) for kw in UNIQUE_KEYWORDS}\n",
        "        \n",
        "        # Create keyword assignments\n",
        "        keyword_assignments = {}\n",
        "        for idx, row_index in enumerate(sorted(random_keyword_indices)):\n",
        "            keyword = UNIQUE_KEYWORDS[idx % len(UNIQUE_KEYWORDS)]\n",
        "            keyword_assignments[row_index] = keyword\n",
        "        \n",
        "        # Show sample assignments\n",
        "        sample_assignments = list(keyword_assignments.items())[:10]\n",
        "        for row_idx, keyword in sample_assignments:\n",
        "            print(f\"   Row {row_idx}: '{keyword}'\")\n",
        "        \n",
        "        # Optimized text assignment (handles all dataset sizes efficiently)\n",
        "        if len(df) > 2_000_000:\n",
        "            # Chunked processing for very large datasets\n",
        "            print(\"   Using chunked processing for memory efficiency...\")\n",
        "            chunk_size = 50_000\n",
        "            \n",
        "            for chunk_start in range(0, len(df), chunk_size):\n",
        "                chunk_end = min(chunk_start + chunk_size, len(df))\n",
        "                \n",
        "                chunk_texts = [\n",
        "                    keyword_texts.get(keyword_assignments.get(i), base_text) \n",
        "                    for i in range(chunk_start, chunk_end)\n",
        "                ]\n",
        "                df.iloc[chunk_start:chunk_end, df.columns.get_loc(input_col)] = chunk_texts\n",
        "                \n",
        "                # Progress reporting every 1M rows\n",
        "                if chunk_start % 1_000_000 == 0 and chunk_start > 0:\n",
        "                    print(f\"     Progress: {chunk_end:,}/{len(df):,} rows ({chunk_end/len(df)*100:.1f}%)\")\n",
        "        else:\n",
        "            # Fast vectorized assignment for smaller datasets\n",
        "            text_values = [keyword_texts.get(keyword_assignments.get(i), base_text) for i in range(len(df))]\n",
        "            df[input_col] = text_values\n",
        "        \n",
        "        print(f\"   ✅ Keywords distributed across {keyword_rows} random rows\")\n",
        "        \n",
        "    print(f\"✅ Data preparation complete: {len(df):,} rows\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare dataset (single optimized function handles all sizes)\n",
        "df_prepared = prepare_data(df_original, TARGET_ROWS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Spans to Arize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from arize.pandas.logger import Client\n",
        "\n",
        "# Configuration for span logging\n",
        "ARIZE_SPACE_ID = os.getenv(\"ARIZE_SPACE_ID\")\n",
        "ARIZE_API_KEY = os.getenv(\"ARIZE_API_KEY\")\n",
        "\n",
        "# Create a unique project name for this benchmark\n",
        "PROJECT_NAME = f\"TextSearchBench-{TARGET_ROWS}-{datetime.now().strftime('%H%M')}\"\n",
        "\n",
        "# Setup Arize client for logging spans\n",
        "arize_client = Client(\n",
        "    space_id=ARIZE_SPACE_ID,\n",
        "    api_key=ARIZE_API_KEY,\n",
        ")\n",
        "\n",
        "print(f\"Arize project name: {PROJECT_NAME}\")\n",
        "print(\"✅ Arize client setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_spans_to_arize(df, batch_size=50_000):\n",
        "    \"\"\"Upload dataframe rows as spans to Arize using log_spans with optimized batching\"\"\"\n",
        "    \n",
        "    total_rows = len(df)\n",
        "    print(f\"\\n=== Uploading {total_rows:,} spans to Arize (Optimized with Batching) ===\")\n",
        "    print(f\"Project: {PROJECT_NAME}\")\n",
        "    print(f\"Batch size: {batch_size:,} rows per batch\")\n",
        "    \n",
        "    # Prepare spans DataFrame efficiently\n",
        "    print(\"\\nPreparing spans DataFrame...\")\n",
        "    with timer():\n",
        "        spans_df = prepare_spans_dataframe(df)\n",
        "        print(f\"   ✅ Prepared {len(spans_df):,} spans with {len(spans_df.columns)} columns\")\n",
        "    \n",
        "    # Batch upload with progress tracking and error handling\n",
        "    print(f\"\\n🚀 Uploading in {(total_rows + batch_size - 1) // batch_size} batches...\")\n",
        "    \n",
        "    successful_batches = 0\n",
        "    failed_batches = 0\n",
        "    total_uploaded = 0\n",
        "    \n",
        "    with timer():\n",
        "        for batch_start in range(0, total_rows, batch_size):\n",
        "            batch_end = min(batch_start + batch_size, total_rows)\n",
        "            batch_num = (batch_start // batch_size) + 1\n",
        "            total_batches = (total_rows + batch_size - 1) // batch_size\n",
        "            \n",
        "            print(f\"\\n📦 Batch {batch_num}/{total_batches}: Uploading rows {batch_start:,}-{batch_end:,}...\")\n",
        "            \n",
        "            # Extract batch\n",
        "            batch_df = spans_df.iloc[batch_start:batch_end].copy()\n",
        "            batch_df.reset_index(drop=True, inplace=True)\n",
        "            \n",
        "            try:\n",
        "                # Upload batch with timeout and retry logic\n",
        "                response = arize_client.log_spans(\n",
        "                    dataframe=batch_df,\n",
        "                    model_id=PROJECT_NAME,\n",
        "                    model_version=\"1.0\",\n",
        "                    validate=True,\n",
        "                    verbose=False  # Reduce noise for batched uploads\n",
        "                )\n",
        "                \n",
        "                if response.status_code == 200:\n",
        "                    successful_batches += 1\n",
        "                    total_uploaded += len(batch_df)\n",
        "                    print(f\"   ✅ Batch {batch_num} uploaded successfully ({len(batch_df):,} spans)\")\n",
        "                else:\n",
        "                    failed_batches += 1\n",
        "                    print(f\"   ❌ Batch {batch_num} failed: {response.status_code} - {response.text[:200]}...\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                failed_batches += 1\n",
        "                print(f\"   ❌ Batch {batch_num} failed with exception: {str(e)[:200]}...\")\n",
        "                \n",
        "            # Progress summary every 10 batches or at the end\n",
        "            if batch_num % 10 == 0 or batch_num == total_batches:\n",
        "                success_rate = (successful_batches / batch_num) * 100\n",
        "                print(f\"   📊 Progress: {total_uploaded:,}/{total_rows:,} spans uploaded ({success_rate:.1f}% success rate)\")\n",
        "    \n",
        "    # Final summary\n",
        "    print(f\"\\n📈 Upload Summary:\")\n",
        "    print(f\"   Total spans: {total_rows:,}\")\n",
        "    print(f\"   Successfully uploaded: {total_uploaded:,}\")\n",
        "    print(f\"   Failed batches: {failed_batches}\")\n",
        "    print(f\"   Success rate: {(successful_batches / ((total_rows + batch_size - 1) // batch_size)) * 100:.1f}%\")\n",
        "    \n",
        "    if total_uploaded == total_rows:\n",
        "        print(f\"   ✅ All spans uploaded successfully to project: {PROJECT_NAME}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"   ⚠️  Partial upload completed. Check failed batches above.\")\n",
        "        return False\n",
        "\n",
        "\n",
        "def prepare_spans_dataframe(df):\n",
        "    \"\"\"Efficiently prepare spans DataFrame with proper data types\"\"\"\n",
        "    # Create clean spans DataFrame with only the required columns\n",
        "    spans_df = pd.DataFrame()\n",
        "    \n",
        "    # Required columns - ensure proper data types\n",
        "    spans_df['context.trace_id'] = df['context.trace_id'].fillna('').astype(str)\n",
        "    spans_df['context.span_id'] = df['context.span_id'].fillna('').astype(str)  \n",
        "    spans_df['name'] = df['name'].fillna('LLM_span').astype(str)\n",
        "    \n",
        "    # Handle timestamps\n",
        "    spans_df['start_time'] = pd.to_datetime(df['start_time'], errors='coerce')\n",
        "    \n",
        "    # Calculate end times using latency_ms\n",
        "    if 'latency_ms' in df.columns:\n",
        "        latency_ms = pd.to_numeric(df['latency_ms'], errors='coerce').fillna(1000.0)\n",
        "        spans_df['end_time'] = spans_df['start_time'] + pd.to_timedelta(latency_ms, unit='ms')\n",
        "    else:\n",
        "        # Generate random latency if not available\n",
        "        latency_ms = [random.uniform(100, 2000) for _ in range(len(df))]\n",
        "        spans_df['end_time'] = spans_df['start_time'] + pd.to_timedelta(latency_ms, unit='ms')\n",
        "    \n",
        "    # Handle input/output attributes - ensure they are strings\n",
        "    if 'attributes.input.value' in df.columns:\n",
        "        spans_df['attributes.input.value'] = df['attributes.input.value'].fillna('').astype(str)\n",
        "    \n",
        "    if 'attributes.output.value' in df.columns:\n",
        "        spans_df['attributes.output.value'] = df['attributes.output.value'].fillna('').astype(str)\n",
        "    \n",
        "    # Handle status code\n",
        "    if 'status_code' in df.columns:\n",
        "        spans_df['status_code'] = df['status_code'].fillna('OK').astype(str)\n",
        "    else:\n",
        "        spans_df['status_code'] = 'OK'\n",
        "    \n",
        "    # Handle span kind\n",
        "    if 'attributes.openinference.span.kind' in df.columns:\n",
        "        spans_df['attributes.openinference.span.kind'] = df['attributes.openinference.span.kind'].fillna('LLM').astype(str)\n",
        "    else:\n",
        "        spans_df['attributes.openinference.span.kind'] = 'LLM'\n",
        "    \n",
        "    # Handle parent_id if present\n",
        "    if 'parent_id' in df.columns:\n",
        "        parent_ids = df['parent_id'].fillna('')\n",
        "        spans_df['parent_id'] = parent_ids.astype(str)\n",
        "        spans_df.loc[spans_df['parent_id'] == '', 'parent_id'] = None\n",
        "    \n",
        "    # Handle token counts if present\n",
        "    for col_mapping in [\n",
        "        ('totalTokenCount', 'attributes.llm.token_count.total'),\n",
        "        ('attributes.llm.token_count.prompt', 'attributes.llm.token_count.prompt'),\n",
        "        ('attributes.llm.token_count.completion', 'attributes.llm.token_count.completion')\n",
        "    ]:\n",
        "        if col_mapping[0] in df.columns:\n",
        "            spans_df[col_mapping[1]] = pd.to_numeric(df[col_mapping[0]], errors='coerce').fillna(0).astype(int)\n",
        "    \n",
        "    return spans_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload the prepared dataset to Arize with optimized batching\n",
        "# Batch size of 50K is optimal for most datasets - adjust if needed\n",
        "upload_spans_to_arize(df_prepared, batch_size=50_000)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
